{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 15)\n",
      "(17653, 9)\n",
      "   age  workclass  education-num  marital-status  capital-gain  capital-loss  \\\n",
      "0   39          5             13               2          2174             0   \n",
      "1   50          1             13               0             0             0   \n",
      "2   38          0              9               1             0             0   \n",
      "\n",
      "   hours-per-week  native-country  50K  \n",
      "0              40               0    0  \n",
      "1              13               0    0  \n",
      "2              40               0    0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12404\\1460382295.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset_path = 'dataset/50Ktrain.csv'\n",
    "data = pd.read_csv(dataset_path)\n",
    "# data = pd.read_csv(dataset_path)\n",
    "print(data.shape)\n",
    "# 篩選資料集，若有缺失值則整row刪除\n",
    "\n",
    "keys_immutable = ['race', 'sex', 'native-country'] \n",
    "\n",
    "# choose data[0] as input feature, for all data points, if the immutable feature of data point isn't same as data[0], then drop it\n",
    "for key in keys_immutable:\n",
    "    data = data[data[key] == data.iloc[0][key]]\n",
    "\n",
    "# drop 'education' and 'fnlwgt' features\n",
    "data = data.drop(['education', 'fnlwgt', 'occupation', 'race', 'sex', 'relationship'], axis=1)\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "data = data.reset_index(drop=True)\n",
    "print(data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12404\\1285118727.py:6: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.05861002  0.75852599 -0.13289512 ... -1.32145659  0.01567507\n",
      " -1.32145659]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  data.iloc[:, :-1] = scaler.fit_transform(data.iloc[:, :-1])\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12404\\1285118727.py:6: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 1.91805278 -0.03476306 -0.52296703 ... -0.52296703 -0.52296703\n",
      " -0.52296703]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  data.iloc[:, :-1] = scaler.fit_transform(data.iloc[:, :-1])\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12404\\1285118727.py:6: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 1.10410909  1.10410909 -0.51067418 ... -0.10697836 -0.51067418\n",
      " -0.51067418]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  data.iloc[:, :-1] = scaler.fit_transform(data.iloc[:, :-1])\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12404\\1285118727.py:6: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 1.28332103 -0.69437135  0.29447484 ...  1.28332103 -0.69437135\n",
      "  1.28332103]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  data.iloc[:, :-1] = scaler.fit_transform(data.iloc[:, :-1])\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12404\\1285118727.py:6: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.08899658 -0.16376676 -0.16376676 ... -0.16376676 -0.16376676\n",
      " -0.16376676]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  data.iloc[:, :-1] = scaler.fit_transform(data.iloc[:, :-1])\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12404\\1285118727.py:6: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.23876906 -0.23876906 -0.23876906 ... -0.23876906 -0.23876906\n",
      " -0.23876906]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  data.iloc[:, :-1] = scaler.fit_transform(data.iloc[:, :-1])\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12404\\1285118727.py:6: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.22240517 -2.41885784 -0.22240517 ... -0.22240517 -0.22240517\n",
      " -1.84940714]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  data.iloc[:, :-1] = scaler.fit_transform(data.iloc[:, :-1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        age  workclass  education-num  marital-status  capital-gain  \\\n",
      "0 -0.058610   1.918053       1.104109        1.283321      0.088997   \n",
      "1  0.758526  -0.034763       1.104109       -0.694371     -0.163767   \n",
      "2 -0.132895  -0.522967      -0.510674        0.294475     -0.163767   \n",
      "\n",
      "   capital-loss  hours-per-week  native-country  50K  \n",
      "0     -0.238769       -0.222405               0    0  \n",
      "1     -0.238769       -2.418858               0    0  \n",
      "2     -0.238769       -0.222405               0    0  \n",
      "\n",
      "        age  workclass  education-num  marital-status  capital-gain  \\\n",
      "0 -1.416313   3.187383      -1.703130        1.283321     -0.161880   \n",
      "1 -0.417916  -0.487569      -0.360660        0.294475     -0.131463   \n",
      "2  0.107413  -0.408269       1.281638       -0.652673     -0.163767   \n",
      "\n",
      "   capital-loss  hours-per-week  native-country  \n",
      "0     -0.238769       -0.620395             0.0  \n",
      "1     -0.238769       -0.223142             0.0  \n",
      "2      4.215229        0.092214             0.0  \n"
     ]
    }
   ],
   "source": [
    "# data standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# standardization all columns except the last column\n",
    "data.iloc[:, :-1] = scaler.fit_transform(data.iloc[:, :-1])\n",
    "# covert to dataframe\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head(3))\n",
    "print()\n",
    "\n",
    "# K-means clustering\n",
    "from sklearn.cluster import KMeans\n",
    "x = df.iloc[:, :-1] # 最後一項不要 (income) 其他去做kmeans\n",
    "model = KMeans(n_clusters=100, n_init='auto', random_state=1).fit(x)\n",
    "\n",
    "# get centers of each cluster\n",
    "centers = model.cluster_centers_\n",
    "# add column name to centers\n",
    "centers = pd.DataFrame(centers, columns=df.columns[:-1])\n",
    "print(centers.head(3))\n",
    "# save centers to csv file\n",
    "np.savetxt('centers.csv', centers, delimiter=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6    7\n",
      "0 -1.416313  3.187383 -1.703130  1.283321 -0.161880 -0.238769 -0.620395  0.0\n",
      "1 -0.417916 -0.487569 -0.360660  0.294475 -0.131463 -0.238769 -0.223142  0.0\n",
      "2  0.107413 -0.408269  1.281638 -0.652673 -0.163767  4.215229  0.092214  0.0\n"
     ]
    }
   ],
   "source": [
    "# read centers from csv file\n",
    "centers = pd.read_csv('centers.csv', header=None)\n",
    "print(centers.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 透過Elbow法找出最佳K值(資料群數)\n",
    "# Dist = []\n",
    "# K = range(1,18)\n",
    "# for k in K:\n",
    "#     kmeanModel = KMeans(n_clusters=k, n_init='auto', random_state=1).fit(x)\n",
    "#     kmeanModel.fit(x)\n",
    "#     Dist.append(kmeanModel.inertia_)\n",
    "# print(Dist)\n",
    "# plt.plot(range(1,18), Dist, 'bx-')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors:\n",
      "              0         1         2         3         4        5\n",
      "15081  0.810470  1.097452 -0.165421 -0.241688 -2.961453 -0.70739\n",
      "10727  1.197121  1.097452 -0.165421 -0.241688 -2.372177 -0.70739\n",
      "8454   0.423820  1.097452 -0.165421 -0.241688 -2.372177 -0.70739\n",
      "4792   0.887800  1.097452 -0.165421 -0.241688 -2.961453 -0.70739\n",
      "2774   0.346489  1.097452 -0.165421 -0.241688 -2.624724 -0.70739\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import queue\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "q = queue.Queue()\n",
    "q.put(1)\n",
    "\n",
    "while not q.empty():\n",
    "    index = q.get()\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Extract features for KNN\n",
    "    features = df[[0, 1, 2, 4]]    # age education-num hours-per-week\n",
    "\n",
    "    # choose data[0] as target data point for which you want to find neighbors\n",
    "    target_data_point = features.iloc[index]\n",
    "\n",
    "    # Create a KNN model\n",
    "    knn_model = NearestNeighbors(n_neighbors=6, algorithm='auto')  # Adjust n_neighbors as needed\n",
    "\n",
    "    # Fit the model with your dataset\n",
    "    knn_model.fit(features)\n",
    "\n",
    "    # Find the indices of the nearest neighbors (excluding the target_data_point itself)\n",
    "    _, indices = knn_model.kneighbors([target_data_point])\n",
    "    neighbor_indices: np.ndarray = indices[0]\n",
    "    neighbor_indices = np.delete(neighbor_indices, np.where(neighbor_indices == index))\n",
    "\n",
    "    # for i in neighbor_indices:\n",
    "    #     q.put(i)\n",
    "\n",
    "    # Print the neighbors\n",
    "    print(\"Nearest neighbors:\")\n",
    "    print(df.iloc[neighbor_indices])\n",
    "\n",
    "    # store the index of the nearest neighbors\n",
    "    neighbor_indices = neighbor_indices.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate a cost function that takes in two data point and returns a cost table with two columns: payment and time\n",
    "payment: sum all\n",
    "time: pick maxium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a cost function that takes in two data point and returns a cost table with two columns: payment and time\n",
    "def generate_cost_table(data_point_1, data_point_2):\n",
    "    # calculate the cost of payment\n",
    "    totaldiscrete = 0\n",
    "    # discrete features\n",
    "    if data_point_1['relationship'] != data_point_2['relationship']:\n",
    "        # use Euclidean distance to calculate the cost of payment\n",
    "        totaldiscrete += np.linalg.norm(data_point_1['relationship'] - data_point_2['relationship'])\n",
    "    \n",
    "    if data_point_1['workclass'] != data_point_2['workclass']:\n",
    "        totaldiscrete += np.linalg.norm(data_point_1['workclass'] - data_point_2['workclass'])\n",
    "    if data_point_1['education'] != data_point_2['education']:\n",
    "        totaldiscrete += np.linalg.norm(data_point_1['education'] - data_point_2['education'])\n",
    "\n",
    "    # calculate the cost of time\n",
    "    totalTime = 0\n",
    "    totalPay = 0\n",
    "    if data_point_1['age'] != data_point_2['age']:\n",
    "        totalTime = max(np.abs(data_point_1['age'] - data_point_2['age']), totalTime)\n",
    "    if data_point_1['education-num'] != data_point_2['education-num']:\n",
    "        totalTime = max(np.abs(data_point_1['education-num'] - data_point_2['education-num']), totalTime)\n",
    "    if data_point_1['capital-gain'] != data_point_2['capital-gain']:\n",
    "        totalPay += (np.abs(data_point_1['capital-gain'] - data_point_2['capital-gain']), totalPay)\n",
    "    if data_point_1['capital-loss'] != data_point_2['capital-loss']:\n",
    "        totalPay -= (np.abs(data_point_1['capital-loss'] - data_point_2['capital-loss']), totalPay)\n",
    "    if data_point_1['hours-per-week'] != data_point_2['hours-per-week']:\n",
    "        totalTime = max(np.abs(data_point_1['hours-per-week'] - data_point_2['hours-per-week']), totalTime)\n",
    "        totalPay += np.abs(data_point_1['hours-per-week'] - data_point_2['hours-per-week'])\n",
    "\n",
    "    #　now we have totaldiscrete、totalTime、totalPay => use a non-linear function to combine them, and then find the parato front\n",
    "    return totaldiscrete, totalTime, totalPay\n",
    "\n",
    "# generate a cost function, and then return 5 point in the parato front( you can change it into \"dominated point\")\n",
    "def generate_cost_table(totaldiscrete, totalTime, totalPay):\n",
    "    # a function: f(totalPay) = totaldiscrete/totalTime\n",
    "    def f(p):\n",
    "        return 1 / p\n",
    "    table = []\n",
    "    while len(table) < 5:\n",
    "        p = np.random.uniform(0, totalPay)\n",
    "        t = f(p)\n",
    "        if t > totalTime:\n",
    "            continue\n",
    "        table.append([p, t])\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch as pt\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "continuous = ['age', 'fnlwgt', 'education-num',\n",
    "              'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "categorical = ['workclass', 'marital-status', 'occupation',\n",
    "               'relationship', 'race', 'sex', 'native-country']\n",
    "target = ['50K']\n",
    "\n",
    "\n",
    "class MLP50K(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_continuous = 6\n",
    "        self.n_categorical = 9, 7, 15, 6, 5, 2, 42\n",
    "        self.n_features = self.n_continuous + sum(self.n_categorical)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.n_features, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x1: pt.Tensor[pt.float], x2: pt.Tensor[pt.int]) -> pt.Tensor:\n",
    "        a = [x1]\n",
    "        for i, n in enumerate(self.n_categorical):\n",
    "            x = nn.functional.one_hot(x2[i], n)\n",
    "            a.append(x)\n",
    "        a = pt.cat(a, 1)\n",
    "        x = self.linear(a)\n",
    "        return x\n",
    "\n",
    "\n",
    "def load_model() -> MLP50K:\n",
    "    path = Path('resource/model.pt')\n",
    "    state = pt.load(path)\n",
    "    model = MLP50K()\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_dataset(which: Literal['train', 'test']) -> pd.DataFrame:\n",
    "    path = Path(f'resource/50K.{which}.csv')\n",
    "    return pd.read_csv(path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mR_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
