{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 15)\n",
      "(32561, 9)\n",
      "   age  workclass  education-num  marital-status  capital-gain  capital-loss  \\\n",
      "0   39          5             13               2          2174             0   \n",
      "1   50          1             13               0             0             0   \n",
      "2   38          0              9               1             0             0   \n",
      "\n",
      "   hours-per-week  native-country  50K  \n",
      "0              40               0    0  \n",
      "1              13               0    0  \n",
      "2              40               0    0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset_path = 'dataset/50Ktrain.csv'\n",
    "data = pd.read_csv(dataset_path)\n",
    "# data = pd.read_csv(dataset_path)\n",
    "print(data.shape)\n",
    "\n",
    "def select_same_immutable(df: pd.DataFrame, index: int) -> pd.DataFrame:\n",
    "    df_col = df[[\"race\", \"sex\", \"native-country\"]]\n",
    "    i = df_col.eq(df_col.iloc[index]).all(1)\n",
    "    return df_col[i]\n",
    "\n",
    "# drop 'education' and 'fnlwgt' features\n",
    "data = data.drop(['education', 'fnlwgt', 'occupation', 'race', 'sex', 'relationship'], axis=1)\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "data = data.reset_index(drop=True)\n",
    "print(data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to modify\n",
    "# knn model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_knn_model(df: pd.DataFrame) -> KNeighborsClassifier:\n",
    "    # split data\n",
    "    X = df.drop('income', axis=1)\n",
    "    y = df['income']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    # train model\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train, y_train)\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(model: KNeighborsClassifier, df: pd.DataFrame) -> np.ndarray:\n",
    "    # given some data, predict the label \n",
    "    model.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# # standardization all columns except the last column\n",
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    data.iloc[:, :-1] = scaler.fit_transform(data.iloc[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6    7\n",
      "0 -1.416313  3.187383 -1.703130  1.283321 -0.161880 -0.238769 -0.620395  0.0\n",
      "1 -0.417916 -0.487569 -0.360660  0.294475 -0.131463 -0.238769 -0.223142  0.0\n",
      "2  0.107413 -0.408269  1.281638 -0.652673 -0.163767  4.215229  0.092214  0.0\n"
     ]
    }
   ],
   "source": [
    "# K-means clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def reduction(df: pd.DataFrame, k: int = 100) -> pd.DataFrame:\n",
    "    x = df.iloc[:, :-1] # 最後一項不要 (income) 其他去做kmeans\n",
    "    model = KMeans(n_clusters=100, n_init='auto', random_state=1).fit(x)\n",
    "\n",
    "    # get centers of each cluster\n",
    "    centers = model.cluster_centers_\n",
    "# add column name to centers\n",
    "    centers = pd.DataFrame(centers, columns=df.columns[:-1])\n",
    "    print(centers.head(3))\n",
    "# save centers to csv file\n",
    "    np.savetxt('centers.csv', centers, delimiter=',')\n",
    "\n",
    "\n",
    "# read centers from csv file\n",
    "centers = pd.read_csv('centers.csv', header=None)\n",
    "print(centers.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 透過Elbow法找出最佳K值(資料群數)\n",
    "# Dist = []\n",
    "# K = range(1,18)\n",
    "# for k in K:\n",
    "#     kmeanModel = KMeans(n_clusters=k, n_init='auto', random_state=1).fit(x)\n",
    "#     kmeanModel.fit(x)\n",
    "#     Dist.append(kmeanModel.inertia_)\n",
    "# print(Dist)\n",
    "# plt.plot(range(1,18), Dist, 'bx-')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors:\n",
      "              0         1         2         3         4        5\n",
      "15081  0.810470  1.097452 -0.165421 -0.241688 -2.961453 -0.70739\n",
      "10727  1.197121  1.097452 -0.165421 -0.241688 -2.372177 -0.70739\n",
      "8454   0.423820  1.097452 -0.165421 -0.241688 -2.372177 -0.70739\n",
      "4792   0.887800  1.097452 -0.165421 -0.241688 -2.961453 -0.70739\n",
      "2774   0.346489  1.097452 -0.165421 -0.241688 -2.624724 -0.70739\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import queue\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "q = queue.Queue()\n",
    "q.put(1)\n",
    "\n",
    "while not q.empty():\n",
    "    index = q.get()\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Extract features for KNN\n",
    "    features = df[[0, 1, 2, 4]]    # age education-num hours-per-week\n",
    "\n",
    "    # choose data[0] as target data point for which you want to find neighbors\n",
    "    target_data_point = features.iloc[index]\n",
    "\n",
    "    # Create a KNN model\n",
    "    knn_model = NearestNeighbors(n_neighbors=6, algorithm='auto')  # Adjust n_neighbors as needed\n",
    "\n",
    "    # Fit the model with your dataset\n",
    "    knn_model.fit(features)\n",
    "\n",
    "    # Find the indices of the nearest neighbors (excluding the target_data_point itself)\n",
    "    _, indices = knn_model.kneighbors([target_data_point])\n",
    "    neighbor_indices: np.ndarray = indices[0]\n",
    "    neighbor_indices = np.delete(neighbor_indices, np.where(neighbor_indices == index))\n",
    "\n",
    "    # for i in neighbor_indices:\n",
    "    #     q.put(i)\n",
    "\n",
    "    # Print the neighbors\n",
    "    print(\"Nearest neighbors:\")\n",
    "    print(df.iloc[neighbor_indices])\n",
    "\n",
    "    # store the index of the nearest neighbors\n",
    "    neighbor_indices = neighbor_indices.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate a cost function that takes in two data point and returns a cost table with two columns: payment and time\n",
    "payment: sum all\n",
    "time: pick maxium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a cost function that takes in two data point and returns a cost table with two columns: payment and time\n",
    "def count_weight(data_point_1, data_point_2):\n",
    "    # calculate the cost of payment\n",
    "    totaldiscrete = 0\n",
    "    # discrete features\n",
    "    if data_point_1['relationship'] != data_point_2['relationship']:\n",
    "        # use Euclidean distance to calculate the cost of payment\n",
    "        totaldiscrete += np.linalg.norm(data_point_1['relationship'] - data_point_2['relationship'])\n",
    "    \n",
    "    if data_point_1['workclass'] != data_point_2['workclass']:\n",
    "        totaldiscrete += np.linalg.norm(data_point_1['workclass'] - data_point_2['workclass'])\n",
    "    if data_point_1['education'] != data_point_2['education']:\n",
    "        totaldiscrete += np.linalg.norm(data_point_1['education'] - data_point_2['education'])\n",
    "\n",
    "    # calculate the cost of time\n",
    "    totalTime = 0\n",
    "    totalPay = 0\n",
    "    if data_point_1['age'] != data_point_2['age']:\n",
    "        totalTime = max(np.abs(data_point_1['age'] - data_point_2['age']), totalTime)\n",
    "    if data_point_1['education-num'] != data_point_2['education-num']:\n",
    "        totalTime = max(np.abs(data_point_1['education-num'] - data_point_2['education-num']), totalTime)\n",
    "    if data_point_1['capital-gain'] != data_point_2['capital-gain']:\n",
    "        totalPay += (np.abs(data_point_1['capital-gain'] - data_point_2['capital-gain']), totalPay)\n",
    "    if data_point_1['capital-loss'] != data_point_2['capital-loss']:\n",
    "        totalPay -= (np.abs(data_point_1['capital-loss'] - data_point_2['capital-loss']), totalPay)\n",
    "    if data_point_1['hours-per-week'] != data_point_2['hours-per-week']:\n",
    "        totalTime = max(np.abs(data_point_1['hours-per-week'] - data_point_2['hours-per-week']), totalTime)\n",
    "        totalPay += np.abs(data_point_1['hours-per-week'] - data_point_2['hours-per-week'])\n",
    "\n",
    "    #　now we have totaldiscrete、totalTime、totalPay => use a non-linear function to combine them, and then find the parato front\n",
    "    return totaldiscrete, totalTime, totalPay\n",
    "\n",
    "# generate a cost function, and then return 5 point in the parato front( you can change it into \"dominated point\")\n",
    "def generate_cost_table(totaldiscrete, totalTime, totalPay):\n",
    "    # a function: f(totalPay) = totaldiscrete/totalTime\n",
    "    def f(p):\n",
    "        return 1 / p\n",
    "    table = []\n",
    "    while len(table) < 5:\n",
    "        p = np.random.uniform(0, totalPay)\n",
    "        t = f(p)\n",
    "        if t > totalTime:\n",
    "            continue\n",
    "        table.append([p, t])\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch as pt\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "continuous = ['age', 'fnlwgt', 'education-num',\n",
    "              'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "categorical = ['workclass', 'marital-status', 'occupation',\n",
    "               'relationship', 'race', 'sex', 'native-country']\n",
    "target = ['50K']\n",
    "\n",
    "\n",
    "class MLP50K(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_continuous = 6\n",
    "        self.n_categorical = 9, 7, 15, 6, 5, 2, 42\n",
    "        self.n_features = self.n_continuous + sum(self.n_categorical)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.n_features, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x1: pt.Tensor[pt.float], x2: pt.Tensor[pt.int]) -> pt.Tensor:\n",
    "        a = [x1]\n",
    "        for i, n in enumerate(self.n_categorical):\n",
    "            x = nn.functional.one_hot(x2[i], n)\n",
    "            a.append(x)\n",
    "        a = pt.cat(a, 1)\n",
    "        x = self.linear(a)\n",
    "        return x\n",
    "\n",
    "\n",
    "def load_model() -> MLP50K:\n",
    "    path = Path('resource/model.pt')\n",
    "    state = pt.load(path)\n",
    "    model = MLP50K()\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_dataset(which: Literal['train', 'test']) -> pd.DataFrame:\n",
    "    path = Path(f'resource/50K.{which}.csv')\n",
    "    return pd.read_csv(path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mR_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
